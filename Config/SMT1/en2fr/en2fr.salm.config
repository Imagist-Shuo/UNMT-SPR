################################################
### CONFIGURATION FILE FOR AN SMT EXPERIMENT ###
################################################


[GENERAL]

home-dir = /home/v-shure #your home directory

working-dir = $home-dir/SMT_WORKPLACE/SMT_1/en-fr
moses-src-dir = $home-dir/mosesdecoder
moses-script-dir = $moses-src-dir/scripts
moses-bin-dir = $moses-src-dir/bin
external-bin-dir = $moses-src-dir/word_align_tools
data-dir = $home-dir/UNMT-SPR/Train/tok/SMT1
train-dir = $home-dir/UNMT-SPR/Config/SMT1/en2fr
#dev-dir = $train-dir/dev
test-dir = $home-dir/UNMT-SPR/Test
#irstlm-dir = $moses-src-dir/irstlm/bin

order = 5

#ttable-binarizer = $moses-bin-dir/processPhraseTableMin
decoder = $moses-bin-dir/moses

# input-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl -l $input-extension"
# output-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl -l $output-extension"
# input-truecaser = $moses-script-dir/recaser/truecase.perl
# output-truecaser = $moses-script-dir/recaser/truecase.perl
# detruecaser = $moses-script-dir/recaser/detruecase.perl


input-extension = en
output-extension = fr
pair-extension = en-fr

#################################################################
# PARALLEL CORPUS PREPARATION: 
# create a tokenized, sentence-aligned corpus, ready for training

[CORPUS]

max-sentence-length = 80

[CORPUS:NewsCrawl] 
tokenized-stem = $data-dir/corpus.$pair-extension.tc

[LM] IGNORE

### tool to be used for language model training
# for instance: ngram-count (SRILM), train-lm-on-disk.perl (Edinburgh) 
# 
# lm-training = "$moses-script-dir/generic/trainlm-irst2.perl -cores 4 -irst-dir $irstlm-dir -temp-dir $working-dir/tmp"
# settings = "-s msb -p 0"
# kenlm training
# lm-training = "$moses-script-dir/ems/support/lmplz-wrapper.perl -bin $moses-bin-dir/lmplz"
# settings = "--prune '0 0 1' -T $working-dir/lm -S 20%"

#order = 5
# lm = $train-dir/lm.bin.$output-extension
# lm-binarizer = $moses-bin-dir/build_binary
#type = 8

# type = 8
# lm-binarizer = $moses-bin-dir/build_binary
# lm-binarizer = $irstlm-dir/compile-lm

[LM:project-syndicate]
type = 8
order = 5
# tokenized-corpus = $train-dir/tc.$pair-extension.$output-extension
lm = $home-dir/Data/unsupervised_new/Models/lm.bin.$output-extension

#################################################################
# TRANSLATION MODEL TRAINING

[TRAINING]

### training script to be used: either a legacy script or 
# current moses training script (default) 
# 
script = $moses-script-dir/training/train-model.perl
training-options = "-mgiza -mgiza-cpus 24 -sort-parallel 24 -cores 24 -parallel"

parallel = yes

### pre-computation for giza++
# giza++ has a more efficient data structure that needs to be
# initialized with snt2cooc. if run in parallel, this may reduces
# memory requirements. set here the number of parts
#
run-giza-in-parts = 5

### symmetrization method to obtain word alignments from giza output
alignment-symmetrization-method = grow-diag-final-and

### lexicalized reordering: specify orientation type
# (default: only distance-based reordering model)
#
lexicalized-reordering = msd-bidirectional-fe

### hierarchical rule set
#
#hierarchical-rule-set = true

max-phrase-length = 5

salm-index = $home-dir/salm/Bin/Linux/Index/IndexSA.O64
sigtest-filter = "-l a+e -n 30"

### if training should be skipped, 
# point to a configuration file that contains
# pointers to all relevant model files
#
#config = 

### TUNING: finding good weights for model components

[TUNING]

tuning-script = $moses-script-dir/training/mert-moses.pl
tuning-settings = "-mertdir $moses-bin-dir "

### specify the corpus used for tuning 
# it should contain 100s if not 1000s of sentences
#
tokenized-input = $test-dir/dev.$pair-extension.tc.$input-extension
 
tokenized-reference = $test-dir/dev.$pair-extension.tc.$output-extension

### size of n-best list used (typically 100)
#
nbest = 100

### ranges for weights for random initialization
# if not specified, the tuning script will use generic ranges
# it is not clear, if this matters
#
# lambda = 

### additional flags for the decoder
#
decoder-settings = "-threads 40"

### if tuning should be skipped, specify this here
# and also point to a configuration file that contains
# pointers to all relevant model files
#
#config = 

#######################################################
## TRUECASER: train model to truecase corpora and input

[TRUECASER] IGNORE

### script to train truecaser models
#
trainer = $moses-script-dir/recaser/train-truecaser.perl

### training data
# raw input needs to be still tokenized,
# also also tokenized input may be specified
#
raw-stem = CORPUS:raw-stem

### trained model
#
#truecase-model = 


##################################
## EVALUATION: score system output

[EVALUATION]

### prepare system output for scoring 
# this may include detokenization and wrapping output in sgm 
# (needed for nist-bleu, ter, meteor)
#
# detokenizer = "$moses-script-dir/tokenizer/detokenizer.perl -l $output-extension"

decoder-settings = "-search-algorithm 1 -cube-pruning-pop-limit 5000 -s 5000 -threads 24"

nbest = 100

### should output be scored case-sensitive (default: no)?
#
# case-sensitive = yes

### BLEU
#

multi-bleu = "$moses-script-dir/generic/multi-bleu.perl -lc"
# ibm-bleu =

### TER: translation error rate (BBN metric) based on edit distance
#
# ter = $edinburgh-script-dir/tercom_v6a.pl

### METEOR: gives credit to stem / worknet synonym matches
#
# meteor = 

[EVALUATION:NewsCrawl]
tokenized-input = $test-dir/test.$pair-extension.tc.$input-extension
tokenized-reference = $test-dir/test.$pair-extension.tc.$output-extension


[REPORTING]

### what to do with result (default: store in file evaluation/report)
# 
# email = pkoehn@inf.ed.ac.uk
